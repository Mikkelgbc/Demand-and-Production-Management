---
title: "Lecture 05 - Forecasting Workshop 2"
author:
  - name: [Mikkel Groth B Christensen]
    affiliation: cand.merc (OSCM)
    
date: "`r Sys.Date()`"
repository_url: [https://github.com/Mikkelgbc/Demand-and-Production-Management.git]
creative_commons: CC BY-NC
output:
  distill::distill_article:
    toc: true
    toc_depth: 3
    toc_float: false
editor_options: 
  chunk_output_type: inline
---

```{r, include=FALSE}
if (interactive()) setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # set working dir to current file location
knitr::opts_chunk$set(
  cache = TRUE, autodep = TRUE,
  echo = TRUE, 
  layout="l-page", fig.width = 12)
```


# Load Packages

```{r}
library(forecast)
library(fpp2)
library(stats)
library(graphics)
library(skimr)
library(gridExtra)
library(rmarkdown)
library(dplyr)
```

# ARIMA Models

## Exercise 1

In this exercise, we use the 'austa' data set, which is the total number of international visitors to Australia (in millions) for the period 1980-2015.

### a) Use auto.arima() to find an appropriate ARIMA model.

```{r}
austa_arima <- auto.arima(austa) %>% 
  print()
```

What model was selected: The most appropriate model is the ARIMA(0,1,1) model with drift.


Check that the residuals look like white noise:

```{r}
checkresiduals(austa_arima)
```

The residuals looks fairly random and normally distributed.


Plot forecasts for the next 10 periods:

```{r}
autoplot(forecast(austa_arima))
```


### b) Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to part a.

```{r}
austa011 = forecast(arima(austa, c(0,1,1)), h=10)
autoplot(austa011)
```

The forecast is not drifting upwards as in a)


Remove the MA term and plot again:

```{r}
austa010 = forecast(arima(austa, c(0,1,0)), h=10)
autoplot(austa010)
```

The confidence interval looks smaller and therefore more accurate.


### c) Plot forecasts from an ARIMA(2,1,3) model with drift.

```{r}
austa213 = forecast(Arima(austa, c(2,1,3),include.drift = TRUE), h=10)
autoplot(austa213)
```

Remove the constant and see what happens:

```{r}

```



### d) Plot forecasts from an ARIMA(0,0,1) model with a constant.

```{r}
austa001 = forecast(arima(austa, c(0,0,1)), include.constant=TRUE, h=10)
autoplot(austa001)
```

Remove the MA term and plot again

```{r}
austa000 = forecast(arima(austa, c(0,0,0)), include.constant=TRUE, h=10)
autoplot(austa000)
```

```{r}
austa021 = forecast(arima(austa, c(0,2,1)), include.constant=FALSE, h=10)
autoplot(austa021)
```

## Exercise 2

In this exercise, we consider the time series sheep, the sheep population of England and Wales from
1867-1939.

```{r}
autoplot(sheep)
```


Assume you decide to fit the model descriped in the text:


What sort of ARIMA model is this (i.e., what are p, d, and q)?

It is ARIMA(3, 1, 0) model.


### b) By examining the ACF and PACF of the dierenced data, explain why this model is appropriate.

```{r}
ggtsdisplay(diff(sheep))
```

ACF plot shows sinusoidally decreasing autocorrelation values, while PACF plot shows significant spikes at lag 1 to 3, but no  beyond lag 3. Therefore ARIMA(3, 1, 0) is appropriate.


### c) Without using the forecast function, calculate forecasts for the next three years (1940{1942).

```{r}
sheep_1940 = 1797 + 0.421 * (1797 - 1791) - 0.202 * (1791 - 1627) - 0.304 * (1627 - 1665)
sheep_1941 = sheep_1940 + 0.421 * (sheep_1940 - 1797) - 0.202 * (1797 - 1791) - 0.304 * (1791 - 1627)
sheep_1942 = sheep_1941 + 0.421 * (sheep_1941 - sheep_1940) - 0.202 * (sheep_1940 - 1797) * 0.304 * (1797-1791)

c(sheep_1940, sheep_1941, sheep_1942)
```

### Fit the model in R and obtain the forecasts using forecast. How are they different from yours? Why?

```{r}
sheep_arima <- forecast(arima(sheep, c(3,1,0)), h=3)
sheep_arima$mean
```

The calculated forecasts were a little bigger than the forecasts from the Arima function.

Small differences in the coefficients made the difference between the first forecast, and then the forecast values were used to calculate the next time point's forecasts, making the difference increasingly bigger.

If we use the coefficients from the arima function, then we can calculate the same forecast values:

```{r}
coef1 <- sheep_arima$model$coef[1]
coef2 <- sheep_arima$model$coef[2]
coef3 <- sheep_arima$model$coef[3]

c(coef1, coef2, coef3)
```

```{r}
sheep.1940.new = 1797 + coef1*(1797 - 1791) + coef2*(1791 - 1627) + coef3*(1627 - 1665)

sheep.1941.new = sheep.1940.new + coef1*(sheep.1940.new - 1797) + coef2*(1797 - 1791) +
  coef3*(1791 - 1627)

sheep.1942.new = sheep.1941.new + coef1*(sheep.1941.new - sheep.1940.new) + 
  coef2*(sheep.1940.new - 1797) + coef3*(1797 - 1791)


c(sheep.1940.new, sheep.1941.new, sheep.1942.new)
```

Above calculation confirms what I said about the differences.


## Exercise 3

In this exercise, we consider the data set 'usmelec', which shows the total net generation of electricity (in billion kilowatt hours) by the U.S. electric industry (monthly for the period January 1973 - June 2013). In general there are two peaks per year: in mid-summer and mid-winter.


<<<<<<< HEAD
### a) Examine the 12-month moving average of this series to see what kind of trend is involved.

```{r}
autoplot(usmelec, ylab = "Generation of electricity", 
    xlab = "Year", 
    main = "Total net generation of electricity (in billion kilowatt hours) by the U.S.")
```

It's hard to see the average trend, due to the seasonality (and probably also cyclicity)


We therefore add a moving average line reflecting the average for each year (MA=12):

```{r}
plot(usmelec, col = "grey") +
  lines(ma(usmelec, order = 12), col = "red")
```

Taking the 12-month moving average makes it easier to see a clear trend. Aside from a noticable dip in the early-mid 1980s there is a consistent increasing linear trend until the late 2000s where it dips and flattens to the end of the data in 2013.


### b) Do the data need transforming? If so, find a suitable transformation.

```{r}
checkresiduals(diff(usmelec))
```

### c) Are the data stationary? If not, find an appropriate differencing which yields stationary data.

The ACF plot is useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.


```{r}
acf(usmelec)
```

It looks like the ACF is slowly decreasing, indicating that this is a non-stationary time series (which is expected, looking at the plot from question a)

We can use he 'ndiffs' function to get an estimate of how many differencing iterations is necessary for making the time series stationary.

```{r}
ndiffs(usmelec)
```

One iteration of differencing is sufficient.


### d) Identify a couple of ARIMA models that might be useful in describing the time series. 

An autoregressive integrated moving average model (ARIMA) examines the differences between values in the series instead of through actual values.

3 components of ARIMA:
Autoregression (AR) - refers to a model that shows a changing variable that regresses on its own lagged, or prior, values.

Integrated (I) represents the differencing of raw observations to allow for the time series to become stationary, i.e., data values are replaced by the difference between the data values and the previous values.

Moving average (MA) incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.


To get a better indication of the data at hand, we make a decomposition of the time series, showing us the trend, seasonality and randomness.

```{r}
plot(decompose(usmelec))
```


Which of your models is the best according to their AIC values?

Using the 'auto.arima' function, we can find the best fitting ARIMA model.

```{r}
usmelec_auto_arima <- auto.arima(usmelec)
usmelec_auto_arima
```

This means that we need to use a Seasonal ARIMA model using 1 lag, with no differencing and double moving average (1,0,2). In the seasonal part is used 0 lag, single differencing and 1 period moving average (0,1,1).

The AIC is 3282,6


### e) Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise?

```{r}
checkresiduals(usmelec_auto_arima)
```

```{r}
summary(usmelec_auto_arima)
```

It looks good.


### f) Forecast the next 15 years of electricity generation by the U.S. electric industry and check the accuracy of your forecasts.

We start by forecasting for the next 15 years using our ARIMA model:

```{r}
usmelec_arima_forecast <- forecast(usmelec_auto_arima, h = 180)

usmelec_arima_forecast %>% 
  as.data.frame() %>% 
  select("Point Forecast") %>% 
  paged_table()
```

```{r}
plot(usmelec_arima_forecast,
     ylab = "Generation of electricity")
lines(ma(usmelec, 12), col = "red")
```


We read in the data for validation:

```{r}
elecData <- readxl::read_excel("Workshop II - Data.xlsx")
elec1120 <-ts(elecData[,"electricity_generation"]/1000,
start = c(1973,1), frequency = 12)
```

# Neural Networks and Validation


## Exercise 4

Calculate the neural network forecast by nnetar() for the time series from the previous exercise. Use a forecasting period of 15 years, the data from usmelec and compare the accuracy to the time series from the Excel file.


We check the proposed neural network:

```{r}
nnetar(usmelec)
```

We make a forecast for the next 15 years using the neural network:

```{r}
usmelec_NN <- forecast(nnetar(usmelec), h=180)

usmelec_NN %>%
  as.data.frame() %>%
  paged_table()
```


We plot the forecast:

```{r}
plot(usmelec_NN)
```

We check the residuals:

```{r}
checkresiduals(usmelec_NN)
```

We check the accuracy of the forecast:

```{r}
accuracy(usmelec_NN)
```

It's seems like a worse forecast looking at the higher RMSE value.


## Exercise 5

In this exercise, we consider the retail data in exercise 2 from workshop 1, and forecast the time series for the next three years.


### a) Develop an appropriate seasonal ARIMA model and forecast

We load the data set (and chose a time series):

```{r}
retaildata <- readxl::read_excel("Australian Retail Data.xlsx", skip=1)
myretail <- ts(retaildata$A3349791W, frequency=12, start=c(1982,4))
```


We find an appropriate ARIMA model

```{r}
auto.arima(myretail)
```

We forecast using the ARIMA model

```{r}
myretail_arima <- forecast(auto.arima(myretail), h=46)

myretail_arima %>% 
  as.data.frame() %>% 
  paged_table()
```

We plot the forecast:

```{r}
plot(myretail_arima)
```

We check the accuracy of the forecast:

```{r}
accuracy(myretail_arima)
```

### b) Create the neural network forecast by using nnetar(). Use the whole data set as training data.

We check the proposed neural network:

```{r}
nnetar(myretail)
```

We forecast the next 3 years using the neural network:

```{r}
myretail_NN <- forecast(nnetar(myretail), h=46)

myretail_NN %>% 
  as.data.frame() %>% 
  paged_table()
```

We plot the forecast:

```{r}
plot(myretail_NN)
```

We check the residuals:

```{r}
checkresiduals(myretail_NN)
```

We chech the accuracy of the forecast:

```{r}
accuracy(myretail_NN)
```

The forecast seems worse looking at the higher RSME value.


### c) Split the data in two parts

The training set should end by December 2010 and the test set start by (January) 2011.

```{r}
training_set <- window(myretail, end=c(2010, 12))
test_set <- window(myretail, end=c(2011))
```


### d) Calculate the forecasts using naive (by snaive(training data)), ARIMA and neural network forecast for the training data.

```{r}
ts_snaive <- snaive(training_set)

ts_snaive %>% 
  as.data.frame() %>% 
  select("Point Forecast") %>%
  paged_table()
```


```{r}
ts_arima <- forecast(auto.arima(training_set))

ts_arima %>% 
  as.data.frame() %>% 
  select("Point Forecast") %>%
  paged_table()
```



```{r}
ts_NN <- forecast(nnetar(training_set))

ts_NN %>% 
  as.data.frame() %>% 
  paged_table()
```

### e) Compare the accuracy between the forecasts (by accuracy(forecast, test data)). What can you see?

```{r}
writeLines("Snaive")
accuracy(ts_snaive)

writeLines("")
writeLines("")
writeLines("ARIMA")
accuracy(ts_arima)

writeLines("")
writeLines("")
writeLines("Neural Network")
accuracy(ts_NN)
```

The neural network seems to make the best forecast with the lowest RMSE value.


### f) Check the residuals. Do the residuals appear to be uncorrelated and normally distributed?

We check the residual for the Snaive:

```{r}
checkresiduals(ts_snaive)
```

We check the residual for the ARIMA:

```{r}
checkresiduals(ts_arima)
```

We check the residual for the Neural Network:

```{r}
checkresiduals(ts_NN)
```

Yes.


### g) How sensitive are the accuracy measures to the training/test split?









